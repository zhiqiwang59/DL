{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiqiwang59/DL/blob/main/2_vlm/vlm_tutorial_practical_1_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuolPkoI7i9X"
      },
      "source": [
        "# M2LS 2025: Vision-Language Models -- Practical 1\n",
        "---\n",
        "- Alexandre Galashov (agalashov@google.com)\n",
        "- Petra Bevandic (Petra.Bevandic@fer.hr )\n",
        "<br>\n",
        "\n",
        "**Abstract:** In this tutorial practical we will focus on Vision Transformer (ViT) as a powerful approach for encoding images.\n",
        "\n",
        "**Disclaimer:** You will mainly be required to complete code blocks which we noted as **\"Your code here\"**. We took care of most of the boilerplate code for you. However, please also feel free to deviate from the code which we prepared and code things in the way you feel is right!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Deep Dive into the Vision Transformer (ViT) üëÅÔ∏è\n",
        "\n",
        "This section is dedicated to the Vision Transformer (ViT), the model that successfully adapted the Transformer architecture for image data.\n",
        "\n",
        "**Our learning objectives are**:\n",
        "\n",
        "* To understand the ViT architecture, with a special focus on the image-to-patch tokenization process.\n",
        "\n",
        "* To implement the key components of a ViT in code.\n",
        "\n",
        "* To train our implementation on a simple image classification task to validate our understanding.\n",
        "\n",
        "The original [ViT](https://arxiv.org/abs/2010.11929) paper is available here for further reading."
      ],
      "metadata": {
        "id": "aSVn37B3-rK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Necessary imports\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "6AoCq_3l0LdP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision transformer Architecture\n",
        "\n",
        "Let's look at the **[Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT)** architecture, illustrated in the diagram. The core idea of ViT is to adapt the successful [Transformer](https://arxiv.org/abs/1706.03762) model, originally from NLP, to process images. This is achieved through a simple, effective process:\n",
        "\n",
        "* **Image Patchification & Embedding**: The input image is first deconstructed into a sequence of fixed-size, non-overlapping patches. You can think of these patches as the visual equivalent of \"words\" üñºÔ∏è. Each patch is then flattened and linearly projected into a vector. This creates a sequence of \"image tokens\" that the Transformer can understand.\n",
        "\n",
        "* **Transformer Encoder**: Finally, this sequence of tokens is fed into a standard Transformer Encoder, which processes the relationships between the patches to understand the image as a whole.\n",
        "\n",
        "* **(Optional) MLP Head**: For a downstream task like image classification, the output representation from the Transformer is passed to a final MLP (Multi-Layer Perceptron) head, which produces the final prediction (e.g., the object class). üéØ\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UxoNoTXy39aIL8RPnpMVbX7wzmXCKlq6\">\n",
        "\n"
      ],
      "metadata": {
        "id": "_9p1CLJLzeC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Pixels to a Sequence: How ViT Processes an Image\n",
        "\n",
        "The first challenge for a Vision Transformer (ViT) is to convert a 2D image into a 1D sequence of vectors, the format a standard Transformer expects. This is done through a clever input pipeline:\n",
        "\n",
        "**Image Patching (Patchification) üñºÔ∏è**. The input image is first divided into a grid of fixed-size, non-overlapping patches. This operation is often called \"patchification,\" and each patch is treated as a single token.\n",
        "\n",
        "* **Example**: A 256x256 pixel image, divided into 16x16 pixel patches, yields a 16x16 grid of patches, for a total of 256 patches.\n",
        "\n",
        "**Linear Embedding üìè**. Each patch is flattened into a long vector and then linearly projected into a consistent vector size, known as the embedding dimension (e.g., 768 dimensions). We now have 256 token embeddings.\n",
        "\n",
        "**Adding Positional Embeddings üìç** The Transformer architecture itself doesn't know the order of tokens. To retain the spatial information of where each patch came from, a learnable positional embedding is added to each patch embedding. This is done in the same way as for word tokens in NLP Transformers.\n",
        "\n",
        "**Prepending the [CLS] Token ‚ûï**. Following the convention from [BERT](https://arxiv.org/abs/1810.04805), an extra learnable embedding‚Äîthe [CLS] (class) token‚Äîis prepended to the start of the sequence. For classification tasks, the final output corresponding to this token is used to represent the entire image.\n",
        "\n",
        "**The Final Sequence ‚úÖ**. The sequence is now ready. It is fed into the Transformer Encoder and consists of the [CLS] token plus the 256 patch embeddings (each with its positional information), making for a total input sequence length of 257 tokens.\n"
      ],
      "metadata": {
        "id": "EmSbRgfQ10Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Original image\n",
        "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "image = image.resize(size=(256, 256))\n",
        "image"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sTgtfM7N-10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let's visualize patches from this image\n",
        "def visualize_patches(image, patch_size):\n",
        "    \"\"\"\n",
        "    Visualizes how an image would be divided into patches.\n",
        "    This is a conceptual visualization and doesn't involve the actual\n",
        "    embedding process, just the division.\n",
        "    \"\"\"\n",
        "    img_width, img_height = image.size\n",
        "    num_patches_w = img_width // patch_size\n",
        "    num_patches_h = img_height // patch_size\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(f\"Original Image with {patch_size}x{patch_size} Patches\")\n",
        "\n",
        "    for i in range(num_patches_w):\n",
        "        for j in range(num_patches_h):\n",
        "            # Draw rectangles to represent patches\n",
        "            rect = plt.Rectangle((i * patch_size, j * patch_size),\n",
        "                                 patch_size, patch_size,\n",
        "                                 linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "conceptual_patch_size = 16\n",
        "visualize_patches(image, conceptual_patch_size)\n",
        "print(f\"The image is conceptually divided into {conceptual_patch_size}x{conceptual_patch_size} patches.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j1yzZmAC0abp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Implement ViT Image Processing\n",
        "\n",
        "Our aim is to implement the ViT's image pre-processing module. The implementation will cover:\n",
        "\n",
        "* **Image Embedding**: Converting image pixels into a sequence of patch vectors.\n",
        "\n",
        "* **Position Embedding**: Injecting spatial information for each patch.\n",
        "\n",
        "* **[CLS] Token**: Adding an optional, global representation token.\n",
        "\n",
        "* **Final Assembly** : Combining these components into the final sequence for the Transformer.\n",
        "\n",
        "Essentially, we will implement the left part of the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UxoNoTXy39aIL8RPnpMVbX7wzmXCKlq6\">"
      ],
      "metadata": {
        "id": "hsvST3A40iSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1**: Implement image patchification & embedding.\n",
        "\n",
        "You need to implement `nn.Module` (see below), which takes an images, splits it into patches and embeds each patch to a hidden dimension.\n",
        "\n",
        "**Tip**: You can use convolution `nn.Conv2d` module to achieve this goal."
      ],
      "metadata": {
        "id": "WGZDwBm9PBfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagePatchificationAndEmbedding(nn.Module):\n",
        "  \"\"\"Splits the image into patches and embeds them.\n",
        "\n",
        "  Receves the transposed images of shape\n",
        "  `<batch_size,in_channels,img_size,img_size>`. It creates patches of size\n",
        "  `patch_size` with a total number of patches equal to\n",
        "  `num_patches=(img_size/patch_size)^2`. Each patch is embedded into the\n",
        "  `embed_dim`. Therefore, it results in a tensor of shape\n",
        "  `<batch_size,num_patches,embed_dim>`.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size:int, patch_size:int, in_channels:int, embed_dim:int):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = (img_size // patch_size) ** 2\n",
        "    self.in_channels = in_channels\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "  def forward(self, transposed_image: torch.Tensor):\n",
        "    x = transposed_image\n",
        "    batch_size = x.shape[0]\n",
        "    assert x.shape == (batch_size, self.in_channels, self.img_size, self.img_size)\n",
        "\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "SHshGxy631xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_patch_embedding():\n",
        "  batch_size = 7\n",
        "  in_channels = 3\n",
        "  img_size = 256\n",
        "  patch_size = 16\n",
        "  embed_dim = 768\n",
        "  transposed_image = torch.ones((batch_size, in_channels, img_size, img_size))\n",
        "  patch_embedding = ImagePatchificationAndEmbedding(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim)\n",
        "  with torch.no_grad():\n",
        "    output = patch_embedding(transposed_image)\n",
        "  num_patches = (img_size // patch_size)**2\n",
        "  if output.shape != (batch_size, num_patches, embed_dim):\n",
        "    raise ValueError(f'The shape of `ImagePatchificationAndEmbedding` output must be`{(batch_size, num_patches, embed_dim)}`, but got `{output.shape}`.')\n",
        "  print('ImagePatchificationAndEmbedding is implemented correctly.')\n",
        "test_patch_embedding()"
      ],
      "metadata": {
        "id": "Nv5LYfan1vd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Implement position embedding"
      ],
      "metadata": {
        "id": "Kkq6aYf6PGD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented `ImagePatchEmbedding`, let's implement position embedding.\n",
        "\n",
        "Checking [transformer](https://arxiv.org/abs/1706.03762) paper, the position embedding (PE) is a tensor of shape `<SEQUENCE_LENGTH, EMBEDDING_DIM>`, where:\n",
        "\n",
        "* `SEQUENCE_LENGTH` is a length of a sequence (in our case it is a number of patches).\n",
        "* `EMBEDDING_DIM` is the embedding dimension, but it must be divisible by 2\n",
        "\n",
        "See the formula below (here $d_{model}$ is the `EMBEDDING_DIM`)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1o9Sx9dJ6DTE2dozaN62eCCb6FvJg3syF\">\n"
      ],
      "metadata": {
        "id": "4cIflQ5B2wXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_position_embedding(sequence_length: int, embed_dim: int):\n",
        "  assert embed_dim % 2 == 0, \"`embed_dim` must be divisible by 2.\"\n",
        "  position_embedding = torch.zeros(sequence_length, embed_dim)\n",
        "  position = torch.arange(0, sequence_length, dtype=torch.float)\n",
        "  position = torch.reshape(position, (sequence_length, 1))\n",
        "\n",
        "  ##############################################################################\n",
        "  # Your code here\n",
        "  ##############################################################################\n",
        "  sin_terms = ...\n",
        "  cos_terms = ...\n",
        "\n",
        "  # All even terms are sin\n",
        "  position_embedding[:, 0::2] = sin_terms\n",
        "  # All odd terms are cos\n",
        "  position_embedding[:, 1::2] = cos_terms\n",
        "\n",
        "  return position_embedding"
      ],
      "metadata": {
        "id": "YLIrs76INmk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_position_embeddings():\n",
        "  sequence_length = 16\n",
        "  embed_dim = 768\n",
        "  position_embedding = compute_position_embedding(sequence_length, embed_dim)\n",
        "  # Check the shapes\n",
        "  if position_embedding.shape != (sequence_length, embed_dim):\n",
        "    raise ValueError(f'`position_embedding` shape must be `{(sequence_length, embed_dim)}` but is {position_embedding.shape}')\n",
        "  # Check that even & odd terms are sin and cos\n",
        "  sin_terms = position_embedding[:, 0::2]\n",
        "  cos_terms = position_embedding[:, 1::2]\n",
        "  num_matches = torch.sum(torch.abs(sin_terms**2 + cos_terms**2 - 1.) <= 1e-5)\n",
        "  if num_matches != sequence_length*(embed_dim // 2):\n",
        "    raise ValueError('Position embedding sin and cos terms should have a sum of squares equal to 1.')\n",
        "  print('Position embedding is implemented correctly.')\n",
        "test_position_embeddings()"
      ],
      "metadata": {
        "id": "njQXBpDpN-fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3**: Final assembly -- full image embedding + `[CLS]` token\n",
        "Now, you are going to implement `ImageEmbedding` modul, which takesan image, does the pathification and embedding, adds positional embedding and optionally a `[CLS]` token."
      ],
      "metadata": {
        "id": "pHd2WW4oPOs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEmbedding(nn.Module):\n",
        "  \"\"\"Embeds image into token embeddings for Vision transformer.\n",
        "\n",
        "  Applies patch embedding, position embedding and adds optionally adds a class embedding.\"\"\"\n",
        "  def __init__(self, img_size: int, patch_size: int, in_channels: int, embed_dim: int, add_cls_token: bool = True):\n",
        "    super().__init__()\n",
        "    # Save arguments\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.in_channels = in_channels\n",
        "    self.embed_dim = embed_dim\n",
        "    self.add_cls_token = add_cls_token\n",
        "\n",
        "    # Patch embedding\n",
        "    self.patch_embedding = ImagePatchificationAndEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "    self.num_patches = self.patch_embedding.num_patches\n",
        "\n",
        "    # Compute num tokens\n",
        "    if add_cls_token:\n",
        "      self.num_tokens = self.num_patches + 1\n",
        "      ##########################################################################\n",
        "      # Your code here\n",
        "      ##########################################################################\n",
        "      ...\n",
        "\n",
        "    else:\n",
        "      self.num_tokens = self.num_patches\n",
        "\n",
        "    # Position embedding\n",
        "    self.position_embeding = compute_position_embedding(self.num_tokens, embed_dim)\n",
        "    self.register_buffer('positional_embedding', self.position_embeding)\n",
        "\n",
        "  def forward(self, transposed_image: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size = transposed_image.shape[0]\n",
        "    # Compute patch embedding\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    tokens_emb = ...\n",
        "    assert tokens_emb.shape == (batch_size, self.num_patches, self.embed_dim)\n",
        "    if self.add_cls_token:\n",
        "      # Add class embedding\n",
        "      ##########################################################################\n",
        "      # Your code here\n",
        "      ##########################################################################\n",
        "      ...\n",
        "\n",
        "    assert tokens_emb.shape == (batch_size, self.num_tokens, self.embed_dim)\n",
        "\n",
        "    # Add position embedding\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    return tokens_emb"
      ],
      "metadata": {
        "id": "QVUWG_8CPtFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_image_embedding():\n",
        "  batch_size = 7\n",
        "  in_channels = 3\n",
        "  img_size = 256\n",
        "  patch_size = 16\n",
        "  embed_dim = 768\n",
        "  transposed_image = torch.ones((batch_size, in_channels, img_size, img_size))\n",
        "  # Not using class embedding\n",
        "  patch_embedding = ImageEmbedding(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    add_cls_token=False)\n",
        "  with torch.no_grad():\n",
        "    output = patch_embedding(transposed_image)\n",
        "  num_patches = (img_size // patch_size)**2\n",
        "  if output.shape != (batch_size, num_patches, embed_dim):\n",
        "    raise ValueError(f'The shape of `ImageEmbedding` output must be`{(batch_size, num_patches, embed_dim)}`, but got `{output.shape}`.')\n",
        "\n",
        "  # Using [CLS] token\n",
        "  patch_embedding = ImageEmbedding(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    add_cls_token=True)\n",
        "  with torch.no_grad():\n",
        "    output = patch_embedding(transposed_image)\n",
        "  num_patches = (img_size // patch_size)**2\n",
        "  if output.shape != (batch_size, num_patches + 1, embed_dim):\n",
        "    raise ValueError(f'The shape of `ImageEmbedding` output must be`{(batch_size, num_patches + 1, embed_dim)}`, but got `{output.shape}`.')\n",
        "\n",
        "  print('ImageEmbedding is implemented correctly.')\n",
        "test_image_embedding()"
      ],
      "metadata": {
        "id": "IVnUkxN2TgN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good job! You have implemented all the image embedding correctly! Now let's see if it works together in ViT."
      ],
      "metadata": {
        "id": "xuDEHy1eT1w_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4**: Run following code -- building blocks for ViT\n",
        "\n",
        "We already pre-implemented ViT for you. Below are the building blocks.\n",
        "\n",
        "Just run the code!"
      ],
      "metadata": {
        "id": "01IlH1Lb5HzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **MLP block**"
      ],
      "metadata": {
        "id": "ChG-dbF7UZa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi-Layer Perceptron for the Transformer block.\"\"\"\n",
        "  def __init__(self, embed_dim: int, mlp_dim: int, dropout_rate: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(embed_dim, mlp_dim)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout1 = nn.Dropout(dropout_rate)\n",
        "    self.fc2 = nn.Linear(mlp_dim, embed_dim)\n",
        "    self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.fc1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.dropout2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_TWdh-fkUcBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Multi-headed attention module**"
      ],
      "metadata": {
        "id": "EfeBjhG94cGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Small Vision Transformer (ViT) Model Definition ---\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  \"\"\"Multi-Head Self-Attention mechanism.\"\"\"\n",
        "  def __init__(self, embed_dim: int, num_heads: int, dropout_rate: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_dim // num_heads\n",
        "    assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, num_tokens, embed_dim = x.shape\n",
        "\n",
        "    # Generate Q, K, V matrices\n",
        "    qkv = self.qkv_proj(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
        "    qkv = qkv.permute(2, 0, 3, 1, 4) # (3, batch_size, num_heads, num_tokens, head_dim)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "    # Scaled Dot-Product Attention\n",
        "    # (batch_size, num_heads, num_tokens, head_dim) @ (batch_size, num_heads, head_dim, num_tokens)\n",
        "    attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # (batch_size, num_heads, num_tokens, head_dim)\n",
        "    output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, embed_dim)\n",
        "\n",
        "    output = self.out_proj(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "n-rQZHHn4bmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Transformer encoder block**"
      ],
      "metadata": {
        "id": "hlH7ueFJ4jyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  \"\"\"Single Transformer Encoder Block.\"\"\"\n",
        "  def __init__(self, embed_dim:int, num_heads:int, mlp_dim:int, dropout_rate: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, dropout_rate)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Mo0KTIWY4jdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 5**: Add ImageEncoder to Vision Transformer (ViT) below\n",
        "\n",
        "Now, we are going to put everything together into `VisionTransformer` module. Your objective is to add `ImageEmbedding` module and to simply embed the images when you call `ViT`."
      ],
      "metadata": {
        "id": "cf5hFT0QUz-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  \"\"\"Small Vision Transformer.\"\"\"\n",
        "  def __init__(self,\n",
        "               img_size:int = 32,\n",
        "               patch_size: int = 4,\n",
        "               in_channels: int = 3,\n",
        "               num_classes: int = 10,\n",
        "               embed_dim: int = 256,\n",
        "               num_heads: int = 8,\n",
        "               num_layers: int = 6,\n",
        "               mlp_dim: int = 512,\n",
        "               dropout_rate: float = 0.1,\n",
        "               add_cls_token: bool = True):\n",
        "      super().__init__()\n",
        "\n",
        "      # Image embedding.\n",
        "      ##########################################################################\n",
        "      # Your code here\n",
        "      ##########################################################################\n",
        "      self.image_embedding = ...\n",
        "\n",
        "\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "      self.transformer_encoder = nn.Sequential(\n",
        "          *[TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)]\n",
        "      )\n",
        "\n",
        "      self.norm = nn.LayerNorm(embed_dim)\n",
        "      self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    tokens_emb = ...\n",
        "\n",
        "    out = self.dropout(tokens_emb)\n",
        "\n",
        "    # Transformer encoder\n",
        "    out = self.transformer_encoder(out)\n",
        "\n",
        "    # Take the output of the class token for classification\n",
        "    out = self.norm(out[:, 0])\n",
        "    logits = self.head(out)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "uO2htRjd_dy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_vit():\n",
        "  batch_size = 13\n",
        "  in_channels = 3\n",
        "  img_size = 32\n",
        "  patch_size = 4\n",
        "  num_classes = 17\n",
        "  transposed_img = torch.ones((batch_size, in_channels, img_size, img_size))\n",
        "  vit = VisionTransformer(in_channels=in_channels, img_size=img_size, patch_size=patch_size, num_classes=num_classes)\n",
        "  with torch.no_grad():\n",
        "    out = vit(transposed_img)\n",
        "  if out.shape != (batch_size, num_classes):\n",
        "    raise ValueError(f'The shape of `VisionTransformer` output must be`{(batch_size, num_classes)}`, but got `{out.shape}`.')\n",
        "  print('VisionTransformer is implemented correctly.')\n",
        "test_vit()\n"
      ],
      "metadata": {
        "id": "2pWtvlIYV-0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 6**: Run training of ViT on a toy dataset\n",
        "\n",
        "Now that we have ViT implemented, we are going to train an image classifier on a toy dataset.\n",
        "\n",
        "There is nothing to implement, just run the code below and check what accuracy you can get!"
      ],
      "metadata": {
        "id": "7NzbSqcYWceP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "zdgfqS3fgRFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTColorDataset(Dataset):\n",
        "  def __init__(self, mnist_split):\n",
        "    self.mnist_split = mnist_split\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mnist_split)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_item = {}\n",
        "    mnist_image = self.mnist_split[idx][0]\n",
        "    batch_item[\"input\"] = torch.cat([mnist_image, mnist_image, mnist_image], axis=0)\n",
        "    batch_item[\"label\"] = torch.LongTensor(np.array([self.mnist_split[idx][1]]))\n",
        "    return batch_item"
      ],
      "metadata": {
        "id": "K4-uvreS_etK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = 10_000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "subset_indices = range(subset_size)\n",
        "mnist_trainset = Subset(mnist_trainset, subset_indices)\n",
        "\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_train = MNISTColorDataset(mnist_trainset)\n",
        "mnist_test = MNISTColorDataset(mnist_testset)\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "aDNkFPy6_hh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = VisionTransformer(\n",
        "    img_size=28,\n",
        "    patch_size=4,\n",
        "    in_channels=3,\n",
        "    num_classes=10,\n",
        "    embed_dim=192,  # Smaller embedding dimension for a \"small\" ViT\n",
        "    num_heads=6,    # Fewer attention heads\n",
        "    num_layers=6,   # Fewer transformer layers\n",
        "    mlp_dim=384,    # Smaller MLP dimension\n",
        "    dropout_rate=0.1\n",
        ").to(device)\n",
        "model.image_embedding.position_embeding = model.image_embedding.position_embeding.to(device)\n",
        "\n",
        "# --- 3. Optimizer and Loss Function ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200) # Cosine annealing scheduler\n",
        "\n",
        "# --- 4. Training and Evaluation Loop ---\n",
        "num_epochs = 5 # You might need more epochs for better performance (e.g., 100-200)"
      ],
      "metadata": {
        "id": "FwgoVE7T_jZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        inputs, labels = batch['input'].to(device), batch['label'][:,0].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    scheduler.step() # Update learning rate\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch['input'].to(device), batch['label'][:,0].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_test_loss = test_loss / len(test_loader)\n",
        "    test_losses.append(epoch_test_loss)\n",
        "    accuracy = 100 * correct / total\n",
        "    test_accuracies.append(accuracy)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
        "          f\"Test Loss: {epoch_test_loss:.4f}, \"\n",
        "          f\"Test Accuracy: {accuracy:.2f}%, \"\n",
        "          f\"Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "print(\"\\nTraining finished!\")"
      ],
      "metadata": {
        "id": "LTVhHyba_ln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Plotting Results ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accuracies, label='Test Accuracy', color='green')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 6. Final Evaluation on Test Set ---\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = batch['input'].to(device), batch['label'][:,0].to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_accuracy = 100 * correct / total\n",
        "print(f\"\\nFinal Accuracy on the 10,000 test images: {final_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "fsttTsGm_mxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see the accuracy around 90\\%, then everything is correct!"
      ],
      "metadata": {
        "id": "unhg7Vhd5iTM"
      }
    }
  ]
}