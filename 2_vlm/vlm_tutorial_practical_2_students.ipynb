{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiqiwang59/DL/blob/main/2_vlm/vlm_tutorial_practical_2_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuolPkoI7i9X"
      },
      "source": [
        "# M2LS 2025: Vision-Language Models -- Practical 2: From Theory to Application ðŸš€\n",
        "---\n",
        "- Alexandre Galashov (agalashov@google.com)\n",
        "- Petra Bevandic (Petra.Bevandic@fer.hr )\n",
        "<br>\n",
        "\n",
        "Now it's time to put our knowledge into action! With the core concepts of **ViT** from Practical 1 under our belts, we're ready to embed image and text into a shared space to solve real-world problems.\n",
        "\n",
        "In this hands-on session, we will focus on understanding CLIP and using it in the following applications:\n",
        "\n",
        "* **Zero-Shot Image Classification**\n",
        "\n",
        "* **Anomaly Detection**\n",
        "\n",
        "* **Image Search**\n",
        "\n",
        "and look at failure cases on Sugar-Crepe benchmark.\n",
        "\n",
        "Let's see what these models can really do!\n",
        "\n",
        "**Disclaimer**: You will mainly be required to complete code blocks which we noted as **\"Your code here\"**. We took care of most of the boilerplate code for you. However, please also feel free to deviate from the code which we prepared and code things in the way you feel is right!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Setup (Hugging Face account)\n",
        "---\n",
        "\n",
        "1. Make a HuggingFace account if you already don't have one (Sign Up).\n",
        "\n",
        "2. Create (if not done so) an access token in HuggingFace.\n",
        "\n",
        "3. Either specify `HF_TOKEN` secret in colab secrets or specify `MANUALLY_ENTERED_HF_TOKEN`."
      ],
      "metadata": {
        "id": "hA1BTzl_g4Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "MANUALLY_ENTERED_HF_TOKEN = '' # If not specified `HF_TOKEN`, enter your token.\n",
        "\n",
        "try:\n",
        "  HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except userdata.SecretNotFoundError:\n",
        "  HF_TOKEN = MANUALLY_ENTERED_HF_TOKEN\n",
        "\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "Yd7UAoxcg4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding CLIP and basics of text-image encoding\n",
        "\n",
        "In this chapter, we'll explore the groundbreaking CLIP model and the art of aligning images with text. By the end of this section, you'll be able to answer:\n",
        "\n",
        "* **ðŸ§  How does CLIP actually learn?** We'll dive deep into the mechanics of its powerful contrastive loss function.\n",
        "\n",
        "* **ðŸš€ How can we use it in practice?** We'll go hands-on, applying pre-trained CLIP embeddings to solve a real-world problem.\n",
        "\n",
        "* **ðŸ¤” What are its limitations?** We'll investigate the common pitfalls and failure modes of contrastive learning to build a more complete understanding."
      ],
      "metadata": {
        "id": "SSfXUGTl2JlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Contrastive Language-Image Pre-training (CLIP)](https://arxiv.org/pdf/2103.00020) is a method that uses large-scale image-text datasets to learn a shared embedding space. In this space, the representations of images and their corresponding text descriptions are close together, while unrelated pairs are pushed far apart.\n",
        "\n",
        "The CLIP model architecture consists of **two encoders**: a **text encoder** and an **image encoder**. These encoders are used to generate representations for text and images, respectively. During training, the model's objective is to predict which image-text pairs within a batch are correctly matched. It achieves this by maximizing the similarity between the embeddings of positive (matching) pairs and minimizing the similarity of negative (mismatching) pairs.\n",
        "\n",
        "CLIP's training approach enables it to perform remarkably well on various image-related tasks, particularly in zero-shot settings where it can classify new images without needing to be fine-tuned on a specific dataset.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bxFxZX7Amwdn4JCyrgZBy87fQDI1yCWM\" height=\"300\" width=\"500\">"
      ],
      "metadata": {
        "id": "oy8Wcher2RRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement the CLIP Loss\n",
        "\n",
        "In this exercise, you will implement the core loss function used in CLIP training. This loss function is crucial for teaching CLIP to align image and text representations effectively. The CLIP loss is defined as follows for a batch of $B$ examples:\n",
        "\n",
        "$$L =   - \\frac{1}{2B} \\sum_{i=1}^{B} \\left( \\log \\frac{  \\exp(\\phi(v_i, t_i) / \\tau)}{\\sum_{j=1}^B\\exp(\\phi(v_i, t_j) / \\tau)} \\right. +  \\left. \\log \\frac{\\exp(\\phi( t_i, v_i) / \\tau)}{\\sum_{j=1}^B\\exp(\\phi(t_i, v_j) / \\tau)} \\right),$$\n",
        "\n",
        "where $\\phi(v_i, t_j) = \\tfrac{v_i}{\\| v_i \\|_2} \\cdot  \\tfrac{t_j}{\\| t_j \\|_2}$ computes the cosine similarity between the image representation ($v_i$) and text representation ($t_i$)  and $\\tau$ is the temperature parameter (that helps control the sharpness of the softmax distribution).\n",
        "\n",
        "Your task is to write a function that calculates the CLIP loss given:\n",
        " - **Image embeddings** (a tensor of shape [batch_size, embedding_dim])\n",
        " - **Text embeddings** (a tensor of shape [batch_size, embedding_dim])\n",
        "\n",
        "The loss function should:\n",
        "\n",
        "1.  **L2 normalize the image embeddings and text embeddings** (this ensures that both image and text embeddings have a unit norm and enables the loss to focus on aligning the semantic directions of images and text, regardless of their initial scales).\n",
        "2.  **Compute the cosine similarity** between each image embedding and each text embedding.\n",
        "3. **Divide the similarity matrix by a temperature scaling factor** (this helps control the sharpness of the softmax distribution).\n",
        "4. **Compute the cross-entropy loss** between the similarity matrix and the target matrix (where the target matrix has 1s along the diagonal, indicating correct image-text pairs).\n",
        "4. Return the **mean loss across the batch**."
      ],
      "metadata": {
        "id": "oRaSodtN2Uzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have implemented the CLIP loss, you can use the following test to check your implementation."
      ],
      "metadata": {
        "id": "EEbUoPCX2NmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "8bGQYMwI2iTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of CLIP loss implementation\n",
        "\n",
        "def clip_loss(image_features, text_features, temperature=0.07):\n",
        "    \"\"\"\n",
        "    Computes the CLIP loss given image and text features.\n",
        "\n",
        "    Args:\n",
        "        image_features (torch.Tensor): Image embeddings of shape (batch_size, embedding_dim).\n",
        "        text_features (torch.Tensor): Text embeddings of shape (batch_size, embedding_dim).\n",
        "        temperature (float, optional): Temperature scaling factor. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The CLIP loss.\n",
        "    \"\"\"\n",
        "\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    # You will need to:\n",
        "    # - Normalize embeddings to unit sphere (L2 normalization)\n",
        "    # - Compute logits: cosine similarities (dot product)\n",
        "    # - Create ground truth labels (positive pairs along the diagonal)\n",
        "    # - Compute cross-entropy loss (symmetrically for both image and text)\n",
        "    # - Average the two losses\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "b5v1pW7F2jCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_clip_loss():\n",
        "  # Test that positive pairs have low loss.\n",
        "  image_features = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
        "  text_features = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
        "  loss = clip_loss(image_features, text_features, temperature=0.07)\n",
        "  assert loss < 0.1  # Small loss for matching pairs\n",
        "\n",
        "  # Test that negative pairs have high loss.\"\"\"\n",
        "  image_features = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
        "  text_features = torch.tensor([[0.0, 1.0], [1.0, 0.0]])  # Mismatched\n",
        "  loss = clip_loss(image_features, text_features, temperature=0.07)\n",
        "  assert loss > 1.0  # High loss for mismatched pairs\n",
        "\n",
        "  return \"Congratulation! Your CLIP loss implementation passes the test.\"\n",
        "test_clip_loss()"
      ],
      "metadata": {
        "id": "TbEu8nQX2pcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP is typically pre-trained on a large dataset of image-text pairs sourced from the internet. This could include images with captions, descriptions, or surrounding text. The initial CLIP model (from OpenAI), for instance, was trained on 400 million image-text pairs.\n",
        "\n",
        "After training, CLIP has learned to map images and text into a shared embedding space where similar concepts are close together and dissimilar concepts are far apart. This allows for zero-shot transfer learning, where the CLIP model can understand the relationship between images and text even for classes it hasn't explicitly seen during training. We will now explore this capability of CLIP pre-trained in the following exercises."
      ],
      "metadata": {
        "id": "t0pZR30k2sio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 - Using CLIP for Semantic Image Search\n",
        "\n",
        "You are now going to build a simple semantic image search tool using a pre-trained CLIP model. This will allow you to enter a text queries (e.g., \"birds in the water\") and retrieve the most relevant images from an image collection."
      ],
      "metadata": {
        "id": "-WpqWMf8Tq1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face's Model Hub provides several pre-trained CLIP models that you can easily download and use."
      ],
      "metadata": {
        "id": "HRyzlBznU26v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipyplot"
      ],
      "metadata": {
        "id": "kvPhe4CEU7vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "import ipyplot\n",
        "import torch"
      ],
      "metadata": {
        "id": "8rgX_GiHU9Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "mPvH_sjhU_T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CLIP model consists of a text_model that computes the text_embeddings for the input texts and a vision model that computes the image_embeddings for the input images."
      ],
      "metadata": {
        "id": "Hx-TQcQ0VDPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model"
      ],
      "metadata": {
        "id": "1yR5Gr6wVIyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clip_processor does all of the necessary image pre-processing (normalization, rescaling, cropping, etc.) and text pre-processing (e.g. tokenization) needed for giving images and text as input to the CLIP model."
      ],
      "metadata": {
        "id": "Nc7BX9KbVGOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_processor"
      ],
      "metadata": {
        "id": "cp3ehaAerZKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Dowload an image collection from pixabay\n",
        "\n",
        "We'll now download a set of images that will represent our image collection for semantic image search. For this purpose, we'll use pixabay and download images of birds (feel free to play around with the pixabay_search_keyword if you want to download a different image collection)."
      ],
      "metadata": {
        "id": "09r1GP-0VKet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_api = \"https://pixabay.com/api/?key=\"\n",
        "pixabay_api_key = \"22176616-358d1b190a298ff59f96b35a1\"\n",
        "\n",
        "pixabay_search_keyword = \"Birds\" #@param {type:\"string\"}\n",
        "\n",
        "no_to_retrieve = 50\n",
        "pixabay_api = original_api+pixabay_api_key+\"&q=\"+pixabay_search_keyword.lower()+\"&image_type=photo&safesearch=true&per_page=\"+str(no_to_retrieve)\n",
        "response = requests.get(pixabay_api)\n",
        "output = response.json()\n",
        "\n",
        "image_collection =[]\n",
        "for each in output[\"hits\"]:\n",
        "    imageurl = each[\"webformatURL\"]\n",
        "    response = requests.get(imageurl)\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    image_collection.append(image)\n",
        "\n",
        "print (\"Total no of images retrived: \",len(image_collection))\n",
        "\n",
        "ipyplot.plot_images(image_collection,max_images=50,img_width=150)"
      ],
      "metadata": {
        "id": "A612LZ_HVPLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1**: Perform Semantic Search\n",
        "\n",
        "Implement a function that uses the CLIP pre-trained models to retrieve the top-k most semantically similar images with a text query."
      ],
      "metadata": {
        "id": "a8XYJoGmVROE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_image_search(text_query, image_collection, top_k=3, clip_processor=clip_processor, clip_model=clip_model):\n",
        "    \"\"\"\n",
        "    Performs semantic image search using CLIP.\n",
        "\n",
        "    Args:\n",
        "        text_query: Text query used to retrieve images.\n",
        "        images_collection: A list of images used for the image search.\n",
        "        top_k: The number of top results to return.\n",
        "        clip_processor: Optional pre-traied CLIPProcessor for pre-processing the input image and text.\n",
        "        clip_model: Optional pre-trained CLIPModel for obtaining image and text embeddings.\n",
        "\n",
        "    Returns:\n",
        "      top_k most similar images for the text_quert\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "      inputs = clip_processor(text=[text_query], images=image_collection, return_tensors=\"pt\", padding=True)\n",
        "      outputs = clip_model(**inputs)\n",
        "    text_emb = outputs.text_embeds\n",
        "    image_emb = outputs.image_embeds\n",
        "\n",
        "    top_k_images = []\n",
        "    ############################################################################\n",
        "    # Your code here\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    return top_k_images"
      ],
      "metadata": {
        "id": "M-zgB3dZVUYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_query = 'bird in the water' #@param {type:\"string\"}\n",
        "top_k = 2 # @param {type:\"integer\"}\n",
        "\n",
        "top_k_images = semantic_image_search(text_query, image_collection, top_k=top_k)\n",
        "\n",
        "ipyplot.plot_images(top_k_images, img_width=300)"
      ],
      "metadata": {
        "id": "BVrJas7aVc68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that semantic-image search represents a zero-shot capability for CLIP as the model has not been directly trained for this task."
      ],
      "metadata": {
        "id": "Pyj_qa1fVftY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Performing Zero-Shot Classification with CLIP\n",
        "\n",
        "In this exercise, we will implement a zero-shot classification pipeline using a pre-trained CLIP model.\n",
        "\n",
        "Core Concept: **Zero-Shot Classification**\n",
        "\n",
        "**Zero-shot classification** refers to the ability of a model to classify data into new categories that it was not explicitly trained on. This powerful technique allows us to bypass the need for task-specific fine-tuning, leveraging the rich, generalized knowledge of a large pre-trained model.\n",
        "\n",
        "Essentially, we will use CLIP \"as-is\" to determine the best-matching category for an image from a list of text labels, without any further training.\n",
        "\n"
      ],
      "metadata": {
        "id": "ppzu1O2hgvIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Classification with CLIP\n",
        "\n",
        "Pre-trained **CLIP (Contrastive Language-Image Pre-training)** can be used for zero-shot classification by leveraging its ability to understand the relationship between images and text descriptions. It was trained on a vast dataset of image-text pairs from the internet to learn a shared representation space where images and their corresponding text descriptions are close together.\n",
        "\n",
        "***\n",
        "\n",
        "#### How It Works\n",
        "\n",
        "1.  **Prepare Candidate Labels**: Instead of training on labeled images, you provide a list of potential class names as text (e.g., \"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\").\n",
        "2.  **Encode Text & Image**: The **text encoder** transforms each of these text labels into a vector. Simultaneously, the **image encoder** converts the input image into a vector in the same space.\n",
        "3.  **Calculate Similarity**: The model then compares the image vector to each of the text vectors using a similarity metric, like cosine similarity.\n",
        "4.  **Predict Class**: The class with the highest similarity score is chosen as the final prediction for the image.\n",
        "\n",
        "***\n",
        "\n",
        "#### Diagram\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/d/1QQETNcX6bYL-CTOqzA4H9dTflA7xabOx\">\n",
        "\n",
        "\n",
        "#### Why It's Powerful\n",
        "\n",
        "**Zero-shot classification with CLIP** is so effective because it doesn't require any retraining or fine-tuning. You can classify images into categories the model has never seen before, as long as you can describe them in text. This makes it incredibly flexible and adaptable for a wide range of tasks.\n",
        "\n",
        "#### Let's do it!\n"
      ],
      "metadata": {
        "id": "NKSGqYI14krf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Necessary imports\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "n81l-m5NRCY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading CLIP model"
      ],
      "metadata": {
        "id": "HPu9pi7s9JFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose CLIP model size.\n",
        "\n",
        "If you choose `SMOL_MODEL=False`, it will load [OpenAI CLIP model](https://huggingface.co/openai/clip-vit-base-patch16) with 150M parameters.\n",
        "\n",
        "If you choose `SMOL_MODEL=True`, it will load [TinyCLIP model](https://arxiv.org/abs/2309.12314), with 25M parameters.\n",
        "\n",
        "You can expect that the big model performs better, but for fast iteration, we recommend using a small model."
      ],
      "metadata": {
        "id": "-a4QY_9jhCVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMOL_MODEL = True #@param"
      ],
      "metadata": {
        "id": "K0Ev0-XrfeWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CLIP model and processor...\")\n",
        "if SMOL_MODEL:\n",
        "  MODEL_ID = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
        "else:\n",
        "  MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
        "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in the model: {pytorch_total_params}\")\n",
        "\n",
        "EMBEDDING_DIM = 512"
      ],
      "metadata": {
        "id": "EDJJjpxOUmv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the model"
      ],
      "metadata": {
        "id": "ufegrvnD9S5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "Ssu6fZpU9SNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What is the final embedding dimension?"
      ],
      "metadata": {
        "id": "34nSqC3x9YQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the dataset\n",
        "\n",
        "We will use [`Imagenette`](https://huggingface.co/datasets/frgfm/imagenette) dataset which is a small version of ImageNet."
      ],
      "metadata": {
        "id": "Pi7dxAMNhcr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loading_transform = transforms.Compose([\n",
        "    processor.image_processor,\n",
        "    lambda x: x['pixel_values'][0]\n",
        "])\n",
        "\n",
        "test_dataset = datasets.Imagenette(\n",
        "    root='./data',\n",
        "    split='val',\n",
        "    size='160px',\n",
        "    download=True,\n",
        "    transform=loading_transform,\n",
        ")\n",
        "\n",
        "print('Imagenette has the following classes: ')\n",
        "imagenette_classes = []\n",
        "for c in test_dataset.classes:\n",
        "  print(f\"{c[0]}\")\n",
        "  imagenette_classes.append(f\"{c[0]}\")\n",
        "\n",
        "NUM_CLASSES = len(imagenette_classes)"
      ],
      "metadata": {
        "id": "NSe3npjESXMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize an image and a class\n",
        "sample_image, sample_class = test_dataset[0]\n",
        "sample_image = torch.Tensor(sample_image).to(DEVICE)\n",
        "\n",
        "image_std = torch.Tensor(processor.image_processor.image_std).reshape((3, 1, 1)).to(DEVICE)\n",
        "image_mean = torch.Tensor(processor.image_processor.image_mean).reshape((3, 1, 1)).to(DEVICE)\n",
        "image_to_plot = sample_image * image_std + image_mean\n",
        "\n",
        "print(f'Class label = `{imagenette_classes[sample_class]}`')\n",
        "plt.imshow(np.transpose(image_to_plot.cpu(), (1, 2, 0)))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o4WYXboldB4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract fixed set of test images and labels\n",
        "test_batch_size = 16 #@param\n",
        "max_test_batches = 30 #@param\n",
        "dataset_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "torch.manual_seed(100)\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for idx, (images, labels) in enumerate(dataset_loader):\n",
        "  test_images.append(images)\n",
        "  test_labels.append(labels)\n",
        "  if idx == max_test_batches - 1:\n",
        "    break"
      ],
      "metadata": {
        "id": "eg0wmxXyE8g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1**: Produce prompts for the classes\n",
        "\n",
        "The CLIP model should receive text as input. We will assign a prompt for each class. Think about what should be a prompt for a class.\n",
        "\n",
        "The following function should produce a list of promts with the same length as the number of classes in a dataset."
      ],
      "metadata": {
        "id": "ZRTGwlAghq9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_class_prompts(class_labels):\n",
        "  class_prompts = []\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "  return class_prompts"
      ],
      "metadata": {
        "id": "m08BOYGSAFpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_prompts = create_class_prompts(test_dataset.classes)\n",
        "assert len(class_prompts) == NUM_CLASSES\n",
        "print('You have created the class prompts')"
      ],
      "metadata": {
        "id": "KNb9TXBoAQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Produce prompt embeddings for all the class prompts for cosine similarity.\n",
        "\n",
        "For every class prompt you have produced, you need to create a corresponding embedding from CLIP model to later use for the cosine similarity.\n",
        "\n",
        "**Note**: You can use `model.get_text_features` function to produce text embeddings.\n",
        "\n",
        "**Note**: Think about what needs to be done for the embeddings to be used for **cosine similarity**.\n",
        "\n",
        "**Note**: You can use `processor` to put `class_prompts` into a correct format. We have already done it for you."
      ],
      "metadata": {
        "id": "ZU_xi4q8ixV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_class_prompts_for_cosine_similarity(class_prompts, model, processor):\n",
        "  with torch.no_grad():\n",
        "    text_inputs = processor(text=class_prompts, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "    ############################################################################\n",
        "    # YOUR CODE HERE\n",
        "    ############################################################################\n",
        "    class_emb = ...\n",
        "  return class_emb"
      ],
      "metadata": {
        "id": "3ytDxd3oAhn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliary test function\n",
        "def check_embedding_for_cosine_similarity(embedding):\n",
        "  num_embeddings = embedding.shape[0]\n",
        "  assert torch.sum(torch.abs(embedding.norm(dim=-1) - 1.0) < 1e-5) == num_embeddings"
      ],
      "metadata": {
        "id": "Ztmp6D9-DOjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Encoding inlier text prompts...\")\n",
        "class_emb_test = embed_class_prompts_for_cosine_similarity(class_prompts, model, processor)\n",
        "print(\"Text prompts encoded.\")\n",
        "\n",
        "assert class_emb_test.shape == (NUM_CLASSES, EMBEDDING_DIM)\n",
        "check_embedding_for_cosine_similarity(class_emb_test)\n",
        "print('You have correctly embedded the classes for cosine similarity.')"
      ],
      "metadata": {
        "id": "Z_bdjDgZR7FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3**: Implement image embedding function\n",
        "\n",
        "Now you need to implement a function which embeds images to be later used for cosine similarity.\n",
        "\n",
        "**Note**: You can use `model.get_image_features` for producing image embeddings, it receives images as `pixel_values`."
      ],
      "metadata": {
        "id": "RXDGTyWxqMNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_images_for_cosine_similarity(images, model):\n",
        "  pixel_values = images.to(DEVICE)\n",
        "  with torch.no_grad():\n",
        "    ############################################################################\n",
        "    # YOUR CODE HERE\n",
        "    ############################################################################\n",
        "    image_emb = ...\n",
        "  return image_emb"
      ],
      "metadata": {
        "id": "9xmzCPbxlrgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 17\n",
        "fake_images = sample_image.reshape((1, 3, 224, 224)).expand((batch_size, -1, -1, -1))\n",
        "image_emb = embed_images_for_cosine_similarity(fake_images, model)\n",
        "\n",
        "assert image_emb.shape == (batch_size, EMBEDDING_DIM)\n",
        "check_embedding_for_cosine_similarity(image_emb)\n",
        "print('You have correctly embedded the images')"
      ],
      "metadata": {
        "id": "0VqLNZlOmOfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4**: Implement function predicting labels\n",
        "\n",
        "Your next task is to implement the classification function. This function will receive two tensors: `image_emb` (with shape `<NUM_IMAGES, EMBEDDING_DIM>`) and `class_emb` (with shape `<NUM_CLASSES, EMBEDDING_DIM>`).\n",
        "\n",
        "Inside the function, you must first compute the similarity between each image and all of the possible class labels. The result will be a **similarity matrix**. After that, your logic should find the most likely class for every image by identifying which class embedding was the most similar to it. The function should then return the predicted class indices."
      ],
      "metadata": {
        "id": "SfQ8bqaKqJWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(image_emb, class_emb):\n",
        "  ############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ############################################################################\n",
        "  ...\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "msWDfhIbn2pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting things together"
      ],
      "metadata": {
        "id": "pVoAEgeqqR-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_zero_shot_clip_accuracy(test_images, test_labels, model, processor, class_prompts):\n",
        "  class_emb = embed_class_prompts_for_cosine_similarity(class_prompts, model, processor)\n",
        "  correct = total = 0\n",
        "  for images, labels in zip(test_images, test_labels):\n",
        "    labels = labels.to(DEVICE)\n",
        "    # Embed current images\n",
        "    image_emb = embed_images_for_cosine_similarity(images, model)\n",
        "    predicted = predict_labels(image_emb, class_emb)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "  accuracy = 100 * correct / total\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "MYPhgmZZFMot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = compute_zero_shot_clip_accuracy(test_images, test_labels, model, processor, class_prompts)\n",
        "print(f\"The accuracy is {accuracy}%\")"
      ],
      "metadata": {
        "id": "UzFDREvjFazm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bonus Task**: Improve Your Classifier\n",
        "\n",
        "Great work so far! If you want to push your results further, here are two key strategies to try:\n",
        "\n",
        "**Upgrade Your CLIP Model**: The performance of your classifier is directly tied to the power of the pre-trained checkpoint. Try loading a larger, more accurate CLIP model and see how it affects your results.\n",
        "\n",
        "**Refine Your Class Prompts**: The default prompt of just the class name (e.g., \"dog\") can be improved. Since CLIP is used to seeing descriptive text, wrapping your labels in a simple template like \"a photo of a dog\" often leads to a significant accuracy boost. Experiment with different templates to find the best one for your dataset."
      ],
      "metadata": {
        "id": "vpZQyRzjEM_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_class_prompts_strategy_a(class_labels):\n",
        "  class_prompts = []\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "  return class_prompts\n",
        "\n",
        "def create_class_prompts_strategy_b(class_labels):\n",
        "  class_prompts = []\n",
        "  ##############################################################################\n",
        "  # YOUR CODE HERE\n",
        "  ##############################################################################\n",
        "  ...\n",
        "  return class_prompts"
      ],
      "metadata": {
        "id": "sbGo2kMfFmC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_prompts_a = create_class_prompts_strategy_a(test_dataset.classes)\n",
        "class_prompts_b = create_class_prompts_strategy_b(test_dataset.classes)\n",
        "accuracy_a = compute_zero_shot_clip_accuracy(test_images, test_labels, model, processor, class_prompts_a)\n",
        "accuracy_b = compute_zero_shot_clip_accuracy(test_images, test_labels, model, processor, class_prompts_b)"
      ],
      "metadata": {
        "id": "lv8vEYnsESxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'Strategy A': (class_prompts_a, accuracy_a),\n",
        "    'Strategy B': (class_prompts_b, accuracy_b),\n",
        "}\n",
        "for key, (class_prompts, accuracy) in results.items():\n",
        "  print(f'{key}: {accuracy}%')\n",
        "  print(f'Class prompts: {[c for c in class_prompts]}')"
      ],
      "metadata": {
        "id": "RijF1pzCHB1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up the memory"
      ],
      "metadata": {
        "id": "br90IjVaAM9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Gk1MIzg-AOff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4 - Out-of-distribution detection with CLIP\n",
        "\n",
        "**Out-of-distribution (OOD) detection** is a task of detecting samples that fall outside of training taxonomy. It is critical for safe deployment of machine learning models in the real world where a model might encounter events not predicted by the training data. For example, road driving datasets will focus on labeling classes that are usually found in traffic scenes (road, vehicles, traffic signs). We, would, however want these models to be able to react to unusual obstacles that could feasibly appear on the road (animals, toys, etc.).\n",
        "\n",
        "\n",
        "Previously, varied negative data was used to improve the ability of of deep models to deal with unknown situations. Vision-language models and their large scale pretraining offer a way to form a good quality feature space that may be utilized to expand the ability of specialized models."
      ],
      "metadata": {
        "id": "Fa1IBFTwqURl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot anomaly detection in the CLIP embedding space\n",
        "\n",
        "In this section, we will implement an anomaly detection system based on a simple and effective technique using CLIP. The goal is to assign a high anomaly score to images that are out-of-distribution (OOD).\n",
        "\n",
        "\n",
        "One simple approach proposed in[ Ming et al, Delving into Out-of Ditribution  \n",
        "Detection, NeuriPS 2022,](https://arxiv.org/pdf/2211.13445) embeds inlier prompts using CLIP. Anomaly score may be expressed as the distance of an image embedding to the closest inlier prompt. It works as follows:\n",
        "\n",
        "* **Define Inlier Prompts**: We create a set of text prompts describing our \"inlier\" (normal) data categories.\n",
        "\n",
        "* **Embed Prompts**: We use CLIP's text encoder to get an embedding for each inlier prompt.\n",
        "\n",
        "* **Calculate Anomaly Score**: For any given test image, we compute its image embedding. The anomaly score is then defined as the distance of this image embedding to the nearest inlier prompt embedding.\n",
        "\n",
        "For this exercise, we will use **MNIST digits as our inlier data** and **FashionMNIST clothing items** as our outliers.\n"
      ],
      "metadata": {
        "id": "BFP_bMMDtMwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Necessary imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "mauCVvqyqiYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading CLIP model"
      ],
      "metadata": {
        "id": "6-VDrLnUHke8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose CLIP model size.\n",
        "\n",
        "If you choose `SMOL_MODEL=False`, it will load [OpenAI CLIP model](https://huggingface.co/openai/clip-vit-base-patch16) with 150M parameters.\n",
        "\n",
        "If you choose `SMOL_MODEL=True`, it will load [TinyCLIP model](https://arxiv.org/abs/2309.12314), with 81M parameters. We use a larger TinyCLIP model here compared to Zero-shot classification because a smaller variant does not work.\n",
        "\n",
        "You can expect that the big model performs better, but for fast iteration, we recommend using a small model."
      ],
      "metadata": {
        "id": "LW3suDoVHke8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMOL_MODEL = True #@param"
      ],
      "metadata": {
        "id": "HIjs7oFJHke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CLIP model and processor...\")\n",
        "if SMOL_MODEL:\n",
        "  MODEL_ID = \"wkcn/TinyCLIP-ViT-39M-16-Text-19M-YFCC15M\"\n",
        "else:\n",
        "  MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
        "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in the model: {pytorch_total_params}\")\n",
        "\n",
        "EMBEDDING_DIM = 512"
      ],
      "metadata": {
        "id": "EJqhAKRZHke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the MNIST and FashionMNIST datasets"
      ],
      "metadata": {
        "id": "o2OojtrtBOx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a subset size.\n",
        "subset_size = 1_000 #@param\n",
        "batch_size = 16 #@param\n",
        "\n",
        "loading_transform = transforms.Compose([\n",
        "    processor.image_processor,\n",
        "    lambda x: x['pixel_values'][0]\n",
        "])\n",
        "\n",
        "print(\"Loading MNIST and Fashion-MNIST test datasets...\")\n",
        "# MNIST is our 'normal' or 'inlier' dataset (label 0)\n",
        "\n",
        "subset_indices = range(subset_size)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=loading_transform)\n",
        "mnist_test = Subset(mnist_test, subset_indices)\n",
        "mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True)\n",
        "# Fashion-MNIST is our 'anomalous' or 'outlier' dataset (label 1)\n",
        "fashion_mnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=loading_transform)\n",
        "fashion_mnist_test = Subset(fashion_mnist_test, subset_indices)\n",
        "fashion_mnist_loader = torch.utils.data.DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=True)\n",
        "print(\"Datasets loaded.\")"
      ],
      "metadata": {
        "id": "EASA_fviqf28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1:** Embed the MNIST class labels to the inlier prompts\n",
        "\n",
        "Think about how to correctly embed the MNIST class labels into inlier prompts. Use your insights from CLIP Zero-shot classification.\n"
      ],
      "metadata": {
        "id": "FFixhhSlFfDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "inlier_prompts = ..."
      ],
      "metadata": {
        "id": "_lmD7hjcq4o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inlier_emb = embed_class_prompts_for_cosine_similarity(inlier_prompts, model, processor)"
      ],
      "metadata": {
        "id": "n-S2KkLcIFW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2:** Calculate anomaly scores for a given set of image embeddings.\n",
        "\n",
        "Your first task is to collect anomaly scores for a given dataset.\n",
        "\n",
        "\n",
        "Similarly to the previous exercise, you will need to embed images into the CLIP feature space, where these embeddings can be compared to the textual descriptions of inlier classes.\n",
        "\n",
        "\n",
        "Unlike the previous exercise, we are not interested in the classification result, but in the confidence/uncertainty of the model in its prediction. This is because lower confidence might indicate that a sample is out-of-distribution.\n",
        "\n",
        "\n",
        "We may model prediction confidence by looking at the maximum cosine similarity between a sample and inlier class descriptions. Anomaly score can then be calculated as `1 - prediction_confidence`."
      ],
      "metadata": {
        "id": "h7fikovMsouI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate Anomaly Scores ---\n",
        "# Anomaly score = 1 - max_cosine_similarity(image_embedding, inlier_text_embeddings) [3]\n",
        "def calculate_anomaly_scores(data_loader, model, text_embeddings):\n",
        "  all_anomaly_scores = []\n",
        "  # Setting up the manual seed.\n",
        "  torch.manual_seed(100)\n",
        "  for i, (images, _) in enumerate(data_loader):\n",
        "    image_emb = embed_images_for_cosine_similarity(images, model)\n",
        "\n",
        "    ############################################################################\n",
        "    # YOUR CODE HERE\n",
        "    # You will need to:\n",
        "    # - Calculate cosine similarity between each image embedding and all text embeddings\n",
        "    # - Find the maximum similarity for each image across all inlier text prompts\n",
        "    # - Anomaly score is 1 - max_similarity (higher score means more anomalous) [3]\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    assert anomaly_scores.shape == (images.shape[0],)\n",
        "    all_anomaly_scores.append(anomaly_scores)\n",
        "  all_anomaly_scores = torch.cat(all_anomaly_scores, dim=0)\n",
        "  return all_anomaly_scores.cpu().numpy()"
      ],
      "metadata": {
        "id": "Rk_UkOCarGjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating anomaly scores for MNIST(inliers)...\")\n",
        "mnist_anomaly_scores = calculate_anomaly_scores(mnist_loader, model, inlier_emb)\n",
        "print(\"\\nCalculating anomaly scores for FashionMNIST (outliers)...\")\n",
        "fashion_mnist_anomaly_scores = calculate_anomaly_scores(fashion_mnist_loader, model, inlier_emb)"
      ],
      "metadata": {
        "id": "YrJyFlc_rIfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3:** Evaluate zero-shot anomaly detection performance.\n",
        "\n",
        "Anomaly detection may be viewed as binary classification between anomalies and inlier samples. This makes binary classification metrics such as [Average Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) and [Area Under Receiver Operating Characteristic Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) suitable to evaluate anomaly detection methods.\n",
        "\n",
        "Your second task is to evaluate the zero shot performance of CLIP for anomaly detection in MNIST."
      ],
      "metadata": {
        "id": "vIZnz2Fis0Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation ---\n",
        "# Calculate AUC-ROC and AUPRC [4, 5]\n",
        "def compute_ood_eval_metrics(inlier_anomaly_score, outlier_anomaly_scores):\n",
        "  ##############################################################################\n",
        "  # YOUR CODE: START\n",
        "  # Combine scores and create true labels\n",
        "  # 0 for inliers (MNIST), 1 for outliers (FashionMNIST)\n",
        "  ##############################################################################\n",
        "  ...\n",
        "\n",
        "  return auc_roc, auprc"
      ],
      "metadata": {
        "id": "5bxF0lGoIfvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_roc, auprc = compute_ood_eval_metrics(mnist_anomaly_scores, fashion_mnist_anomaly_scores)\n",
        "print(f\"\\n--- Anomaly Detection Performance ---\")\n",
        "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "print(f\"AUPRC: {auprc:.4f}\")"
      ],
      "metadata": {
        "id": "CtQcW9mKI3d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization (Optional) ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(mnist_anomaly_scores, bins=50, alpha=0.7, label='MNIST (Inliers)', color='blue', density=True)\n",
        "plt.hist(fashion_mnist_anomaly_scores, bins=50, alpha=0.7, label='Fashion MNIST (Outliers)', color='red', density=True)\n",
        "plt.title('Distribution of Anomaly Scores')\n",
        "plt.xlabel('Anomaly Score (1 - Max Cosine Similarity)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RAUUyCCgrK0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If \"red\" distribution is shifted to the right compared to the \"blue\" distribution -> You have done everything correctly!"
      ],
      "metadata": {
        "id": "FckVXrKuE8px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus Task: Improve OOD detection\n",
        "\n",
        "Great work so far! If you want to push your results further, here are two key strategies to try:\n",
        "\n",
        "**Upgrade Your CLIP Model**: The performance of your classifier is directly tied to the power of the pre-trained checkpoint. Try loading a larger, more accurate CLIP model and see how it affects your results.\n",
        "\n",
        "**Refine Your Inlinear Prompts**: Try different ways of incoding inlinear prompts.\n",
        "\n",
        "**Different anonaly score**: Try different ways of computing anomaly score."
      ],
      "metadata": {
        "id": "BMpPKMQEFRQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "inlier_prompts_new = ... # TO FILL\n",
        "\n",
        "inlier_emb_new = embed_class_prompts_for_cosine_similarity(inlier_prompts_new, model, processor)"
      ],
      "metadata": {
        "id": "k0QISCdGGV2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_anomaly_scores_new(data_loader, model, text_embeddings):\n",
        "  all_anomaly_scores = []\n",
        "  # Setting up the manual seed.\n",
        "  torch.manual_seed(100)\n",
        "  for i, (images, _) in enumerate(data_loader):\n",
        "    image_emb = embed_images_for_cosine_similarity(images, model)\n",
        "    ############################################################################\n",
        "    # YOUR CODE: START\n",
        "    # You will need to:\n",
        "    # - Calculate cosine similarity between each image embedding and all text embeddings\n",
        "    # - Find the maximum similarity for each image across all inlier text prompts\n",
        "    # - Anomaly score is 1 - max_similarity (higher score means more anomalous) [3]\n",
        "    ############################################################################\n",
        "    ...\n",
        "\n",
        "    assert anomaly_scores.shape == (images.shape[0],)\n",
        "    all_anomaly_scores.append(anomaly_scores)\n",
        "  all_anomaly_scores = torch.cat(all_anomaly_scores, dim=0)\n",
        "  return all_anomaly_scores.cpu().numpy()"
      ],
      "metadata": {
        "id": "OvcUb4AFHV3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating anomaly scores for MNIST(inliers)...\")\n",
        "mnist_anomaly_scores_new = calculate_anomaly_scores_new(mnist_loader, model, inlier_emb_new)\n",
        "print(\"\\nCalculating anomaly scores for FashionMNIST (outliers)...\")\n",
        "fashion_mnist_anomaly_scores_new = calculate_anomaly_scores_new(fashion_mnist_loader, model, inlier_emb_new)"
      ],
      "metadata": {
        "id": "Dv_7xuZ0GQNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_roc_new, auprc_new = compute_ood_eval_metrics(mnist_anomaly_scores_new, fashion_mnist_anomaly_scores_new)\n",
        "print(f\"\\n--- Anomaly Detection Performance (New) ---\")\n",
        "print(f\"AUC-ROC: {auc_roc_new:.4f}\")\n",
        "print(f\"AUPRC: {auprc_new:.4f}\")\n",
        "\n",
        "print(f\"\\n--- Anomaly Detection Performance (Old) ---\")\n",
        "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "print(f\"AUPRC: {auprc:.4f}\")"
      ],
      "metadata": {
        "id": "wEWVx85PJAWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(mnist_anomaly_scores, bins=50, alpha=0.7, label='MNIST (Inliers) -- old', color='blue', density=True)\n",
        "plt.hist(mnist_anomaly_scores_new, bins=50, alpha=0.9, label='MNIST (Inliers) -- new', color='green', density=True)\n",
        "plt.hist(fashion_mnist_anomaly_scores, bins=50, alpha=0.7, label='Fashion MNIST (Outliers) -- old', color='red', density=True)\n",
        "plt.hist(fashion_mnist_anomaly_scores_new, bins=50, alpha=0.9, label='Fashion MNIST (Outliers) -- new', color='yellow', density=True)\n",
        "plt.title('Distribution of Anomaly Scores')\n",
        "plt.xlabel('Anomaly Score (1 - Max Cosine Similarity)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HAqM69ycGf5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gret job!"
      ],
      "metadata": {
        "id": "M86Urp-rJd0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: CLIP failure cases, SugarCrepe benchmark\n",
        "\n",
        "We will now use the [Sugar Crepe benchmark ](https://arxiv.org/pdf/2306.14610) to dive deeper into the CLIP model and better understand its failure cases."
      ],
      "metadata": {
        "id": "7tOi2pq8Vo2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets --upgrade"
      ],
      "metadata": {
        "id": "jP5K-ZgTVzCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"HuggingFaceM4/SugarCrepe_swap_att\")"
      ],
      "metadata": {
        "id": "qR92fhm9VzuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now inspect the images and text in this benchmark."
      ],
      "metadata": {
        "id": "8wR2zn7QV3jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = dataset['test']['image']\n",
        "all_candidate_captions = dataset['test']['tested_labels'] # For each image we have 2 candidate captions."
      ],
      "metadata": {
        "id": "VT1uZZ74V1zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipyplot.plot_images(all_images,max_images=10,img_width=150)"
      ],
      "metadata": {
        "id": "WDnQNseEV5y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_candidate_captions[0:10]"
      ],
      "metadata": {
        "id": "WYaMI6FRV7Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Rank the candidate text captions for each image\n",
        "\n",
        "\n",
        "Each image in this dataset has 2 potential caption candidates. Use a CLIP pre-trained model to rank the caption candidates. Return the ranked captions and their scores."
      ],
      "metadata": {
        "id": "gk2qXmauV88D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_image_captions(image, candidate_captions, clip_processor, clip_model):\n",
        "  \"\"\"\n",
        "    Rank candidate captions for an image using CLIP.\n",
        "\n",
        "    Args:\n",
        "        image: Input image.\n",
        "        candidate_captions: A list of possible captions for the image.\n",
        "        clip_processor: Pre-traied CLIPProcessor for pre-processing the input image and text.\n",
        "        clip_model: Pre-trained CLIPModel for obtaining image and text embeddings.\n",
        "\n",
        "    Returns:\n",
        "        ranked_captions: Candidate captions ranked by the similarity with the image.\n",
        "        ranked_scores: Ranked similarity scores.\n",
        "    \"\"\"\n",
        "  with torch.no_grad():\n",
        "    inputs = clip_processor(text=candidate_captions, images=[image], return_tensors=\"pt\", padding=True)\n",
        "    outputs = clip_model(**inputs)\n",
        "  text_emb = outputs.text_embeds\n",
        "  image_emb = outputs.image_embeds\n",
        "  ##############################################################################\n",
        "  # Your code here\n",
        "  ##############################################################################\n",
        "  ...\n",
        "  return output"
      ],
      "metadata": {
        "id": "e8xYPH8bWCGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using your implementation let's now visualize which captions have the highest score for each image."
      ],
      "metadata": {
        "id": "6nAfc3QbWElx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "max_examples = 10\n",
        "\n",
        "for image, candidate_captions in zip(all_images[:max_examples], all_candidate_captions[:max_examples]):\n",
        "  ranked_captions, scores = rank_image_captions(image, candidate_captions, clip_processor, clip_model)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "  print (f'{ranked_captions[0]} Score: {scores[0]}')\n",
        "  print (f'{ranked_captions[1]} Score: {scores[1]}')"
      ],
      "metadata": {
        "id": "Xj-jfKXxWGL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice here? Is the most similar caption (i.e. the highest scoring caption) the correct one?"
      ],
      "metadata": {
        "id": "DvwWV-KNWIVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discuss: CLIP failure case\n",
        "\n",
        "By aligning a global image representation with a global text representation the CLIP model often fails to distinguish between different object attributes (e.g. color understanding) and other finegrained details in the image/text. This is due to the fact that these are often not needed to solve the contrastive loss. To address this shortcoming, additional losses (e.g. captioniong) need to be added to the vision language model."
      ],
      "metadata": {
        "id": "hKC2_RZFWLV3"
      }
    }
  ]
}